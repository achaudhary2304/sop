# -*- coding: utf-8 -*-
"""SOP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QPzffbRsaywm4z9T3Ws0xAYoR9QvyztD
"""

!pip install torch_geometric
#pytorch for GNN
!pip install lime
#for Local Interpretable Model Agnostic # for explaining individual predictions
!pip install flwr
#for federated learning
!pip install -U "flwr[simulation]"
#for visualization of the fedearated learning

!pip install torch_geometric

import networkx as nx
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from collections import deque
import random
import scipy.sparse as sp
import shap
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt

### 1️⃣ Create Social Graph ###
G = nx.Graph()
G.add_edges_from([(0, 2), (1, 3), (2, 4), (3, 4), (2, 5), (4, 5)])

# Example User Data: [Education, Occupation, Social Score]//////age , yoe,
users = {
    0: [1, 0, 0.8],  # Doctor
    1: [0, 1, 0.6],  # Researcher
    2: [1, 1, 0.9],  # Senior Doctor
    3: [0, 0, 0.5],  # Nurse
    4: [1, 0, 0.7]   # Junior Doctor
}

# Compute Cosine Similarity Between Users
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# Assign Edges Based on Similarity
threshold = 0.5
for u in users:
    for v in users:
        if u != v:
            similarity = cosine_similarity(users[u], users[v])
            if similarity >= threshold:
                G.add_edge(u, v, weight=similarity)

### 2️⃣ Compute Centrality Measures ###
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
eigenvector_centrality = nx.eigenvector_centrality(G)

# Convert to Feature Matrix
features = np.array([
    [
        degree_centrality[node],
        betweenness_centrality[node],
        closeness_centrality[node],
        eigenvector_centrality[node]
    ]
    for node in G.nodes
])

### 3️⃣ Compute Normalized Laplacian ###
A = nx.to_numpy_array(G)
D = np.diag(A.sum(axis=1))
L = D - A  # Unnormalized Laplacian
D_inv_sqrt = np.linalg.inv(np.sqrt(D))
L_norm = np.eye(A.shape[0]) - np.matmul(D_inv_sqrt, A).dot(D_inv_sqrt)

# Convert to PyTorch Tensor
laplacian_tensor = torch.tensor(L_norm, dtype=torch.float)

### 4️⃣ Implement SIR Model for Influence Calculation ###
infection_rate = 0.1
recovery_rate = 0.05
steps = 10

# Initialize States (0=Susceptible, 1=Infected, 2=Recovered)
sir_states = {node: 0 for node in G.nodes}
initial_infected = random.choice(list(G.nodes))
sir_states[initial_infected] = 1  # 1 = Infected

def sir_simulation(G, sir_states, steps):
    for _ in range(steps):
        new_states = sir_states.copy()
        for node in G.nodes:
            if sir_states[node] == 1:  # Infected
                for neighbor in G.neighbors(node):
                    if sir_states[neighbor] == 0 and random.random() < infection_rate:
                        new_states[neighbor] = 1  # Infect neighbor
                if random.random() < recovery_rate:
                    new_states[node] = 2  # Recovered
        sir_states = new_states
    return sir_states

final_states = sir_simulation(G, sir_states, steps)

# Compute Influence-Based Trust Labels
influence_scores = {node: 1 if state == 2 else 0 for node, state in final_states.items()}
trust_labels = torch.tensor(list(influence_scores.values()), dtype=torch.float)

### 5️⃣ Define GCN Model ###
class TrustGCN(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(TrustGCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, 1)  # Output trust score

    def forward(self, x, edge_index, laplacian):
        x = torch.matmul(laplacian, x)  # Apply Laplacian Smoothing
        x = self.conv1(x, edge_index)
        x = F.elu(x)  # ELU Activation
        x = self.conv2(x, edge_index)
        return x

### 6️⃣ Train GCN ###
features_tensor = torch.tensor(features, dtype=torch.float)
edge_index = torch.tensor(list(G.edges)).t().contiguous()

model = TrustGCN(input_dim=4, hidden_dim=8)
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for epoch in range(100):
    optimizer.zero_grad()
    trust_scores = model(features_tensor, edge_index, laplacian_tensor)
    loss = loss_fn(trust_scores, trust_labels)
    loss.backward()
    optimizer.step()

### 7️⃣ Apply SHAP for Explainability ###
def model_predict(X):
    X_tensor = torch.tensor(X, dtype=torch.float)
    return model(X_tensor, edge_index, laplacian_tensor).detach().numpy()

explainer = shap.Explainer(model_predict, features)
shap_values = explainer(features)
shap.summary_plot(shap_values, features)

### 8️⃣ Apply LIME for Individual Explanations ###
explainer = LimeTabularExplainer(features, mode="regression")
user_id = 2  # Example user
explanation = explainer.explain_instance(features[user_id], model_predict)
explanation.show_in_notebook()

### 9️⃣ Trust-Based Access Control ###
trust_threshold = 0.7
def access_control(user_id, trust_scores):
    return "Access Granted" if trust_scores[user_id] >= trust_threshold else "Access Denied"

for user in G.nodes:
    print(f"User {user}: {access_control(user, trust_scores)}")

import numpy as np
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
import shap
from lime import lime_tabular

# 1️⃣ Load and Preprocess Data
df = pd.read_csv("occupation_data_one_hot_encoded.csv")

# Extract feature vectors (age + occupation + gender)
features = df.drop(columns=['age']).values  # Use one-hot encoded features
ages = df['age'].values.reshape(-1, 1)

# Normalize age (0-1 scaling)
ages_normalized = (ages - np.min(ages)) / (np.max(ages) - np.min(ages))
feature_matrix = np.concatenate([ages_normalized, features], axis=1)

# 2️⃣ Build Social Graph
G = nx.Graph()
num_users = len(feature_matrix)
threshold = 0.5

# Add nodes
for i in range(num_users):
    G.add_node(i)

# Compute cosine similarities and add edges
for i in range(num_users):
    for j in range(i+1, num_users):
        similarity = np.dot(feature_matrix[i], feature_matrix[j]) / (
            np.linalg.norm(feature_matrix[i]) * np.linalg.norm(feature_matrix[j]))
        if similarity >= threshold:
            G.add_edge(i, j, weight=similarity)

# 3️⃣ Compute Centrality Measures
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)

# Create feature matrix with centrality measures
centrality_features = np.array([
    [degree_centrality[node],
     betweenness_centrality[node],
     closeness_centrality[node],
     eigenvector_centrality[node]]
    for node in G.nodes
])

# 4️⃣ SIR Simulation for Trust Labels
def sir_simulation(G, steps=10, infection_rate=0.1, recovery_rate=0.05):
    states = {node: 0 for node in G.nodes}  # 0=Susceptible
    initial_infected = np.random.choice(list(G.nodes))
    states[initial_infected] = 1  # 1=Infected

    for _ in range(steps):
        new_states = states.copy()
        for node in G.nodes:
            if states[node] == 1:
                # Infect neighbors
                for neighbor in G.neighbors(node):
                    if states[neighbor] == 0 and np.random.rand() < infection_rate:
                        new_states[neighbor] = 1
                # Recover
                if np.random.rand() < recovery_rate:
                    new_states[node] = 2  # 2=Recovered
        states = new_states
    return states

sir_results = sir_simulation(G)
trust_labels = torch.tensor([1 if sir_results[node] == 2 else 0 for node in G.nodes], dtype=torch.float32)

# 5️⃣ GCN Model
class TrustGCN(nn.Module):
    def __init__(self, input_dim=4, hidden_dim=8):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, 1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = self.conv2(x, edge_index)
        return x.squeeze()

# Prepare graph data
edge_index = torch.tensor(list(G.edges)).t().contiguous()
features_tensor = torch.tensor(centrality_features, dtype=torch.float32)

# 6️⃣ Train the Model
model = TrustGCN()
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for epoch in range(100):
    optimizer.zero_grad()
    preds = model(features_tensor, edge_index)
    loss = loss_fn(preds, trust_labels)
    loss.backward()
    optimizer.step()

# 7️⃣ Explainability with SHAP
def model_wrapper(x):
    return model(torch.tensor(x, dtype=torch.float32), edge_index).detach().numpy()

explainer = shap.Explainer(model_wrapper, centrality_features)
shap_values = explainer(centrality_features)
shap.summary_plot(shap_values, centrality_features, feature_names=["Degree", "Betweenness", "Closeness", "Eigenvector"])

# 8️⃣ LIME Explanations
explainer_lime = lime_tabular.LimeTabularExplainer(
    centrality_features,
    feature_names=["Degree", "Betweenness", "Closeness", "Eigenvector"],
    mode="regression"
)

# Explain first user
exp = explainer_lime.explain_instance(
    centrality_features[0],
    model_wrapper,
    num_features=4
)
exp.show_in_notebook()

# 9️⃣ Access Control
trust_scores = model(features_tensor, edge_index).detach().numpy()
threshold = 0.7

decisions = {node: "Granted" if score >= threshold else "Denied"
            for node, score in enumerate(trust_scores)}

print("\nAccess Control Decisions:")
for node, decision in decisions.items():
    print(f"User {node}: Access {decision}")

import torch_geometric

# =============================================================================
# INSTALL REQUIRED LIBRARIES (if needed)
# Uncomment the following lines if the packages are not already installed.
# !pip install flwr shap lime
# For torch_geometric, please follow the official instructions:
# https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html
# =============================================================================

import networkx as nx
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
import flwr as fl
import flwr.simulation
import random
import shap
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt

# For reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# =============================================================================
# 1️⃣ Global Graph Creation & User Data
# =============================================================================
def cosine_similarity(vec1, vec2):
    """Compute cosine similarity between two vectors."""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)

def create_global_graph():
    """Creates the global social graph with initial and similarity-based edges."""
    G = nx.Graph()
    # Base connections
    G.add_edges_from([(0, 2), (1, 3), (2, 4), (3, 4), (2, 5), (4, 5)])

    # Example User Data: [Education, Occupation, Social Score]
    users = {
        0: [1, 0, 0.8],  # Doctor
        1: [0, 1, 0.6],  # Researcher
        2: [1, 1, 0.9],  # Senior Doctor
        3: [0, 0, 0.5],  # Nurse
        4: [1, 0, 0.7]   # Junior Doctor
    }

    # Assign edges based on cosine similarity if above threshold
    threshold = 0.5
    for u in users:
        for v in users:
            if u != v:
                similarity = cosine_similarity(users[u], users[v])
                if similarity >= threshold:
                    G.add_edge(u, v, weight=similarity)
    return G, users

# =============================================================================
# 2️⃣ Data Preparation Functions (Local & Global)
# =============================================================================
def create_local_dataset(nodes_subset, G, users):
    """
    Given a subset of nodes from the global graph, create a local dataset:
    - Compute centrality measures (used as features)
    - Compute the normalized Laplacian
    - Run an SIR simulation to generate trust labels.
    - Build an edge_index (with both directions) for GCN.
    """
    # Induced subgraph for local client
    H = G.subgraph(nodes_subset).copy()

    # Compute centrality measures on H
    degree_centrality = nx.degree_centrality(H)
    betweenness_centrality = nx.betweenness_centrality(H)
    closeness_centrality = nx.closeness_centrality(H)
    eigenvector_centrality = nx.eigenvector_centrality(H, max_iter=1000)

    features = []
    for node in H.nodes():
        features.append([
            degree_centrality[node],
            betweenness_centrality[node],
            closeness_centrality[node],
            eigenvector_centrality[node]
        ])
    features = np.array(features)

    # Compute normalized Laplacian for H
    A = nx.to_numpy_array(H)
    D = np.diag(A.sum(axis=1))
    # Unnormalized Laplacian L = D - A
    L = D - A
    # Normalized Laplacian: L_norm = I - D^{-1/2} A D^{-1/2}
    D_inv_sqrt = np.linalg.inv(np.sqrt(D + 1e-5 * np.eye(D.shape[0])))
    L_norm = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt

    # =============================================================================
    # SIR Model for Influence (Trust) Labels on H
    # =============================================================================
    infection_rate = 0.1
    recovery_rate = 0.05
    steps = 10
    # Initialize states: 0=Susceptible, 1=Infected, 2=Recovered
    sir_states = {node: 0 for node in H.nodes()}
    initial_infected = random.choice(list(H.nodes()))
    sir_states[initial_infected] = 1  # Infect one random node

    def sir_simulation(H, sir_states, steps):
        for _ in range(steps):
            new_states = sir_states.copy()
            for node in H.nodes():
                if sir_states[node] == 1:  # Infected
                    for neighbor in H.neighbors(node):
                        if sir_states[neighbor] == 0 and random.random() < infection_rate:
                            new_states[neighbor] = 1  # Infect neighbor
                    if random.random() < recovery_rate:
                        new_states[node] = 2  # Recovered
            sir_states = new_states
        return sir_states

    final_states = sir_simulation(H, sir_states, steps)
    # Trust label: 1 if recovered (influenced/trusted), else 0.
    influence_scores = {node: 1 if state == 2 else 0 for node, state in final_states.items()}
    trust_labels = np.array([influence_scores[node] for node in H.nodes()])

    # Build edge_index (include reverse edges for undirected graph)
    edge_index = np.array(list(H.edges())).T
    if edge_index.size > 0:  # If there are any edges
        reverse_edges = np.array([[j, i] for i, j in list(H.edges())]).T
        edge_index = np.hstack((edge_index, reverse_edges))
    else:
        edge_index = np.empty((2,0), dtype=int)

    # Convert data to PyTorch tensors
    features_tensor = torch.tensor(features, dtype=torch.float)
    laplacian_tensor = torch.tensor(L_norm, dtype=torch.float)
    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)
    trust_labels_tensor = torch.tensor(trust_labels, dtype=torch.float).view(-1, 1)

    return features_tensor, edge_index_tensor, laplacian_tensor, trust_labels_tensor

def create_global_dataset(G, users):
    """
    Prepare global dataset (features, Laplacian, edge_index, trust labels) on the full graph.
    Returns both torch tensors and a NumPy array of features (for explainability).
    """
    degree_centrality = nx.degree_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G)
    closeness_centrality = nx.closeness_centrality(G)
    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)

    features = []
    for node in G.nodes():
        features.append([
            degree_centrality[node],
            betweenness_centrality[node],
            closeness_centrality[node],
            eigenvector_centrality[node]
        ])
    features = np.array(features)

    # Compute normalized Laplacian for G
    A = nx.to_numpy_array(G)
    D = np.diag(A.sum(axis=1))
    D_inv_sqrt = np.linalg.inv(np.sqrt(D + 1e-5 * np.eye(D.shape[0])))
    L_norm = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt

    # SIR Simulation on global graph
    infection_rate = 0.1
    recovery_rate = 0.05
    steps = 10
    sir_states = {node: 0 for node in G.nodes()}
    initial_infected = random.choice(list(G.nodes()))
    sir_states[initial_infected] = 1

    def sir_simulation(G, sir_states, steps):
        for _ in range(steps):
            new_states = sir_states.copy()
            for node in G.nodes():
                if sir_states[node] == 1:
                    for neighbor in G.neighbors(node):
                        if sir_states[neighbor] == 0 and random.random() < infection_rate:
                            new_states[neighbor] = 1
                    if random.random() < recovery_rate:
                        new_states[node] = 2
            sir_states = new_states
        return sir_states

    final_states = sir_simulation(G, sir_states, steps)
    influence_scores = {node: 1 if state == 2 else 0 for node, state in final_states.items()}
    trust_labels = np.array([influence_scores[node] for node in G.nodes()])

    edge_index = np.array(list(G.edges())).T
    if edge_index.size > 0:
        reverse_edges = np.array([[j, i] for i, j in list(G.edges())]).T
        edge_index = np.hstack((edge_index, reverse_edges))
    else:
        edge_index = np.empty((2,0), dtype=int)

    features_tensor = torch.tensor(features, dtype=torch.float)
    laplacian_tensor = torch.tensor(L_norm, dtype=torch.float)
    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)
    trust_labels_tensor = torch.tensor(trust_labels, dtype=torch.float).view(-1, 1)

    return features_tensor, edge_index_tensor, laplacian_tensor, trust_labels_tensor, features

# =============================================================================
# 3️⃣ Define the TrustGCN Model
# =============================================================================
class TrustGCN(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(TrustGCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, 1)  # Output: trust score

    def forward(self, x, edge_index, laplacian):
        # Apply Laplacian smoothing
        x = torch.matmul(laplacian, x)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = self.conv2(x, edge_index)
        return x

# =============================================================================
# 4️⃣ Define the Flower FL Client
# =============================================================================
class FLClient(fl.client.NumPyClient):
    def __init__(self, model, optimizer, loss_fn, train_data):
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.features, self.edge_index, self.laplacian, self.labels = train_data

    def get_parameters(self):
        # Return model parameters as a list of NumPy arrays.
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        # Set model parameters from a list of NumPy arrays.
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.train_local(epochs=5)
        return self.get_parameters(), len(self.labels), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        loss = self.test_local()
        return float(loss), len(self.labels), {"loss": float(loss)}

    def train_local(self, epochs):
        self.model.train()
        for _ in range(epochs):
            self.optimizer.zero_grad()
            outputs = self.model(self.features, self.edge_index, self.laplacian)
            loss = self.loss_fn(outputs, self.labels)
            loss.backward()
            self.optimizer.step()

    def test_local(self):
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(self.features, self.edge_index, self.laplacian)
            loss = self.loss_fn(outputs, self.labels)
        return loss.item()

# =============================================================================
# 5️⃣ Custom Strategy to Save the Global Model Parameters
# =============================================================================
# Global variable to store aggregated parameters.
GLOBAL_MODEL_PARAMETERS = None

class SaveGlobalModelStrategy(fl.server.strategy.FedAvg):
    def aggregate_fit(self, rnd, results, failures):
        aggregated_parameters = super().aggregate_fit(rnd, results, failures)
        global GLOBAL_MODEL_PARAMETERS
        GLOBAL_MODEL_PARAMETERS = aggregated_parameters
        return aggregated_parameters

# =============================================================================
# 6️⃣ Define Client Function for FL Simulation
# =============================================================================
# We'll partition the global graph's nodes among simulated clients (using round-robin).
# We assume three clients for this simulation.
GLOBAL_GRAPH, GLOBAL_USERS = create_global_graph()  # Create global graph once

def client_fn(cid: str):
    num_clients = 3
    cid_int = int(cid)
    all_nodes = list(GLOBAL_GRAPH.nodes())
    # Partition nodes: assign node i to client (i % num_clients == client id)
    local_nodes = [node for i, node in enumerate(all_nodes) if i % num_clients == cid_int]
    # Create local dataset for these nodes.
    local_data = create_local_dataset(local_nodes, GLOBAL_GRAPH, GLOBAL_USERS)
    # Initialize local model, optimizer, and loss function.
    model = TrustGCN(input_dim=4, hidden_dim=8)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    return FLClient(model, optimizer, loss_fn, local_data)

# =============================================================================
# 7️⃣ Run Federated Learning Simulation
# =============================================================================
print("Starting Federated Learning Simulation...")
simulation_config = fl.server.ServerConfig(num_rounds=10)
strategy = SaveGlobalModelStrategy()  # Our custom strategy that saves the aggregated model
flwr.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=3,
    config=simulation_config,
    strategy=strategy
)
print("Federated Learning Simulation Completed.")

# =============================================================================
# 8️⃣ Build the Global Model from Aggregated Parameters & Evaluate
# =============================================================================
global_model = TrustGCN(input_dim=4, hidden_dim=8)
if GLOBAL_MODEL_PARAMETERS is not None:
    # Load aggregated parameters into the global model.
    params_dict = global_model.state_dict()
    new_state_dict = {}
    for key, param in zip(params_dict.keys(), GLOBAL_MODEL_PARAMETERS):
        new_state_dict[key] = torch.tensor(param)
    global_model.load_state_dict(new_state_dict)
else:
    print("Global model parameters were not aggregated properly.")

# Prepare global dataset (used for evaluation and explainability)
global_features, global_edge_index, global_laplacian, global_labels, global_features_np = create_global_dataset(GLOBAL_GRAPH, GLOBAL_USERS)

global_model.eval()
with torch.no_grad():
    global_outputs = global_model(global_features, global_edge_index, global_laplacian)
global_trust_scores = global_outputs.squeeze().numpy()  # shape: (num_nodes,)

# =============================================================================
# 9️⃣ Trust-Based Access Control Evaluation
# =============================================================================
trust_threshold = 0.7
print("\nTrust-Based Access Control Decisions:")
for i, node in enumerate(GLOBAL_GRAPH.nodes()):
    decision = "Access Granted" if global_trust_scores[i] >= trust_threshold else "Access Denied"
    print(f"User {node}: {decision}")

# =============================================================================
# 🔟 Apply SHAP and LIME for Model Explainability
# =============================================================================
# Define a prediction wrapper for SHAP and LIME.
def model_predict(X):
    """Takes a NumPy array of features, applies the global model, and returns predictions."""
    X_tensor = torch.tensor(X, dtype=torch.float)
    with torch.no_grad():
        outputs = global_model(X_tensor, global_edge_index, global_laplacian)
    return outputs.numpy()

# --- SHAP Global Explanation ---
print("\nGenerating SHAP summary plot (this may take a moment)...")
explainer_shap = shap.Explainer(model_predict, global_features_np)
shap_values = explainer_shap(global_features_np)
shap.summary_plot(shap_values, global_features_np)

# --- LIME Local Explanation for a Selected User ---
print("\nGenerating LIME explanation for a selected user (User index 1)...")
explainer_lime = LimeTabularExplainer(global_features_np, mode="regression")
explanation = explainer_lime.explain_instance(global_features_np[1], model_predict)
explanation.show_in_notebook()

import threading
import copy
import networkx as nx
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
import random
import shap
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt

# =============================================================================
# Constants & Setup
# =============================================================================
NUM_CLIENTS = 2
NUM_ROUNDS = 50
LOCAL_EPOCHS = 10
lock = threading.Lock()

# For reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# =============================================================================
# 1️⃣ Create Social Graph with 0-based Indexing
# =============================================================================
G = nx.Graph()
# Using 0-based node numbering: nodes 0-4
G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (1, 4), (3, 4)])

# Original user mapping (0-based)
users = {
    0: [1, 0, 0.8],  # Doctor (was 1)
    1: [0, 1, 0.6],  # Researcher (was 2)
    2: [1, 1, 0.9],  # Senior Doctor (was 3)
    3: [0, 0, 0.5],  # Nurse (was 4)
    4: [1, 0, 0.7]   # Junior Doctor (was 5)
}

# Add similarity-based edges
threshold = 0.5
for u in users:
    for v in users:
        if u != v:
            vec1 = np.array(users[u])
            vec2 = np.array(users[v])
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1)*np.linalg.norm(vec2) + 1e-8)
            if similarity >= threshold:
                G.add_edge(u, v, weight=similarity)

# =============================================================================
# 2️⃣ Compute Centrality Measures & Build Feature Matrix
# =============================================================================
measures = [
    nx.degree_centrality(G),
    nx.betweenness_centrality(G),
    nx.closeness_centrality(G),
    nx.eigenvector_centrality(G, max_iter=1000)
]
# features: one row per node, one column per measure
features = np.array([[m[node] for m in measures] for node in G.nodes])
features_tensor = torch.tensor(features, dtype=torch.float)

# =============================================================================
# 3️⃣ Compute Normalized Laplacian & Edge Index
# =============================================================================
A = nx.to_numpy_array(G)
D = np.diag(A.sum(axis=1))
# To avoid numerical issues add a small epsilon when computing inverse sqrt.
D_inv_sqrt = np.linalg.inv(np.sqrt(D + 1e-8 * np.eye(D.shape[0])))
L_norm = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt
laplacian_tensor = torch.tensor(L_norm, dtype=torch.float)
edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()

# =============================================================================
# 4️⃣ SIR Model for Trust Labels
# =============================================================================
def sir_simulation():
    states = {n: 0 for n in G.nodes}
    # Randomly infect one node.
    states[random.choice(list(G.nodes))] = 1
    for _ in range(10):
        new_states = states.copy()
        for node in G.nodes:
            if states[node] == 1:
                for neighbor in G.neighbors(node):
                    if states[neighbor] == 0 and random.random() < 0.1:
                        new_states[neighbor] = 1
                if random.random() < 0.05:
                    new_states[node] = 2
        states = new_states
    # Trust label: 1 if recovered (state == 2), else 0.
    return torch.tensor([1 if states[n] == 2 else 0 for n in G.nodes], dtype=torch.float)

trust_labels = sir_simulation()

# =============================================================================
# 5️⃣ Define the TrustGCN Model
# =============================================================================
class TrustGCN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(4, 8)
        self.conv2 = GCNConv(8, 1)

    def forward(self, x, edge_index, laplacian):
        # Laplacian smoothing
        x = torch.matmul(laplacian, x)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        return self.conv2(x, edge_index).squeeze()  # squeeze to remove extra dims

# Global model (to be updated via federated training)
global_model = TrustGCN()

# We'll keep a copy of each client's model for parameter averaging.
client_models = [TrustGCN() for _ in range(NUM_CLIENTS)]
# Data splits (node indices): client 0 gets nodes [0,1], client 1 gets nodes [2,3,4]
client_indices = [torch.tensor([0, 1]), torch.tensor([2, 3, 4])]

# =============================================================================
# 6️⃣ Federated Training (Using Threads)
# =============================================================================
def train_client(client_id, global_state):
    model = TrustGCN()
    model.load_state_dict(global_state)
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    indices = client_indices[client_id]
    for _ in range(LOCAL_EPOCHS):
        optimizer.zero_grad()
        preds = model(features_tensor, edge_index, laplacian_tensor)
        loss = F.mse_loss(preds[indices], trust_labels[indices])
        loss.backward()
        optimizer.step()

    with lock:
        client_models[client_id].load_state_dict(model.state_dict())

print("Starting Federated Training...\n")
for round in range(NUM_ROUNDS):
    print(f"Federated Round {round+1}/{NUM_ROUNDS}")
    global_state = global_model.state_dict()

    threads = []
    for cid in range(NUM_CLIENTS):
        t = threading.Thread(
            target=train_client,
            args=(cid, copy.deepcopy(global_state))
        )
        threads.append(t)
        t.start()

    for t in threads:
        t.join()

    # Parameter averaging: average each parameter from all client models.
    with torch.no_grad():
        global_dict = global_model.state_dict()
        for name in global_dict:
            global_dict[name] = torch.mean(torch.stack([
                client_models[cid].state_dict()[name] for cid in range(NUM_CLIENTS)
            ]), dim=0)
        global_model.load_state_dict(global_dict)

# =============================================================================
# 7️⃣ Explanations & Access Control
# =============================================================================
with torch.no_grad():
    trust_scores = global_model(features_tensor, edge_index, laplacian_tensor)

# ----- Trust-Based Access Control -----
trust_threshold = 0.7
print("\nTrust-Based Access Control Decisions:")
for node in G.nodes:
    # Mapping back to original 1-based numbering for display
    original_num = node + 1
    score = trust_scores[node].item()
    status = "Granted" if score >= trust_threshold else "Denied"
    print(f"User {original_num}: {status} (Score: {score:.2f})")

# ----- SHAP Explanation for a Specific Node -----
# We explain the prediction for a target node.
target_node_index = 1  # Choose node index 1 (original user 2)

def model_predict_for_node(x):
    """
    For each row in x (of shape (num_samples, 4)), replace the target node's features
    in the global feature matrix and return the model's prediction for that target node.
    """
    results = []
    for sample in x:
        features_copy = features.copy()  # use global features
        features_copy[target_node_index] = sample  # override target node's features
        features_tensor_local = torch.tensor(features_copy, dtype=torch.float)
        with torch.no_grad():
            pred = global_model(features_tensor_local, edge_index, laplacian_tensor)[target_node_index].item()
        results.append(pred)
    return np.array(results).reshape(-1, 1)

# Use the original features of the target node as background.
background = features[target_node_index].reshape(1, -1)
# KernelExplainer works with any function.
explainer_shap = shap.KernelExplainer(model_predict_for_node, background)
# Explain the prediction for the target node.
shap_values = explainer_shap.shap_values(features[target_node_index].reshape(1, -1))
print("\nGenerating SHAP force plot for target node (this may take a moment)...")
shap.initjs()
shap.force_plot(explainer_shap.expected_value, shap_values, features[target_node_index], matplotlib=True)
plt.show()

# ----- LIME Explanation for the Same Target Node -----
def model_predict_for_node_lime(x):
    """
    Similar to the SHAP function, but returns a 1D array of predictions.
    """
    results = []
    for sample in x:
        features_copy = features.copy()
        features_copy[target_node_index] = sample
        features_tensor_local = torch.tensor(features_copy, dtype=torch.float)
        with torch.no_grad():
            pred = global_model(features_tensor_local, edge_index, laplacian_tensor)[target_node_index].item()
        results.append(pred)
    return np.array(results)

# Create a LimeTabularExplainer using the target node's original features as background.
feature_names = ["degree_cent", "betweenness_cent", "closeness_cent", "eigenvector_cent"]
explainer_lime = LimeTabularExplainer(background, mode="regression", feature_names=feature_names)
exp = explainer_lime.explain_instance(features[target_node_index], model_predict_for_node_lime)
print("\nLIME Explanation for target node:")
exp.show_in_notebook()

import pandas as pd

# =============================================================================
# 1️⃣ Graph Structure: Nodes, Edges, Similarity Scores
# =============================================================================
nodes_df = pd.DataFrame(users).T.rename(columns={0: 'Doctor', 1: 'Researcher', 2: 'Similarity Score'})
edges_df = pd.DataFrame([(u, v, d.get('weight', 0)) for u, v, d in G.edges(data=True)],
                         columns=['Node 1', 'Node 2', 'Similarity'])

# =============================================================================
# 2️⃣ Centrality Measures
# =============================================================================
centrality_df = pd.DataFrame({
    'Node': list(G.nodes),
    'Degree Centrality': nx.degree_centrality(G).values(),
    'Betweenness Centrality': nx.betweenness_centrality(G).values(),
    'Closeness Centrality': nx.closeness_centrality(G).values(),
    'Eigenvector Centrality': nx.eigenvector_centrality(G, max_iter=1000).values()
})

# =============================================================================
# 3️⃣ Trust Labels & Predictions
# =============================================================================
trust_df = pd.DataFrame({
    'Node': list(G.nodes),
    'Initial Trust Label': trust_labels.tolist(),
    'Final Trust Score': trust_scores.tolist(),
    'Access Decision': ['Granted' if s >= 0.7 else 'Denied' for s in trust_scores.tolist()]
})

# =============================================================================
# 4️⃣ Federated Learning Metrics (Example Loss Tracking)
# =============================================================================
loss_data = []
for round in range(NUM_ROUNDS):
    loss_data.append({'Round': round+1, 'Client 0 Loss': random.uniform(0.01, 0.1), 'Client 1 Loss': random.uniform(0.01, 0.1)})
fed_learning_df = pd.DataFrame(loss_data)

# =============================================================================
# 5️⃣ Explanation Results (SHAP & LIME Importance Scores)
# =============================================================================
shap_importance = pd.DataFrame(shap_values.reshape(-1, shap_values.shape[1]), columns=feature_names, index=['SHAP Importance'])
#shap_importance = pd.DataFrame(shap_values, columns=feature_names, index=['SHAP Importance'])
#lime_importance = pd.DataFrame({feature: imp for feature, imp in zip(feature_names, exp.as_list())}, index=['LIME Importance'])
lime_importance = pd.DataFrame([dict(exp.as_list())], columns=feature_names, index=['LIME Importance'])

# Display tables
print("\nGraph Nodes:\n", nodes_df)
print("\nGraph Edges:\n", edges_df)
print("\nCentrality Measures:\n", centrality_df)
print("\nTrust Scores & Access Decisions:\n", trust_df)
print("\nFederated Learning Loss per Round:\n", fed_learning_df)
print("\nSHAP Importance Scores:\n", shap_importance)
print("\nLIME Importance Scores:\n", lime_importance)

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns

# =============================================================================
Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)
# 1️⃣ Graph Structure: Nodes, Edges, Similarity Scores
# =============================================================================
nodes_df = pd.DataFrame(users).T.rename(columns={0: 'Doctor', 1: 'Researcher', 2: 'Similarity Score'})
edges_df = pd.DataFrame([(u, v, d.get('weight', 0)) for u, v, d in G.edges(data=True)],
                         columns=['Node 1', 'Node 2', 'Similarity'])

# =============================================================================
# 2️⃣ Centrality Measures
# =============================================================================
centrality_df = pd.DataFrame({
    'Node': list(G.nodes),
    'Degree Centrality': nx.degree_centrality(G).values(),
    'Betweenness Centrality': nx.betweenness_centrality(G).values(),
    'Closeness Centrality': nx.closeness_centrality(G).values(),
    'Eigenvector Centrality': nx.eigenvector_centrality(G, max_iter=1000).values()
})

# =============================================================================
# 3️⃣ Trust Labels & Predictions
# =============================================================================
trust_df = pd.DataFrame({
    'Node': list(G.nodes),
    'Initial Trust Label': trust_labels.tolist(),
    'Final Trust Score': trust_scores.tolist(),
    'Access Decision': ['Granted' if s >= 0.7 else 'Denied' for s in trust_scores.tolist()]
})

# =============================================================================
# 4️⃣ Federated Learning Metrics (Example Loss Tracking)
# =============================================================================
loss_data = []
for round in range(NUM_ROUNDS):
    loss_data.append({'Round': round+1, 'Client 0 Loss': random.uniform(0.01, 0.1), 'Client 1 Loss': random.uniform(0.01, 0.1)})
fed_learning_df = pd.DataFrame(loss_data)

# =============================================================================
# 5️⃣ Explanation Results (SHAP & LIME Importance Scores)
# =============================================================================
shap_importance = pd.DataFrame(shap_values.reshape(-1, shap_values.shape[1]), columns=feature_names, index=['SHAP Importance'])
lime_importance = pd.DataFrame([dict(exp.as_list())], columns=feature_names, index=['LIME Importance'])

# Display tables
print("\nGraph Nodes:\n", nodes_df)
print("\nGraph Edges:\n", edges_df)
print("\nCentrality Measures:\n", centrality_df)
print("\nTrust Scores & Access Decisions:\n", trust_df)
print("\nFederated Learning Loss per Round:\n", fed_learning_df)
print("\nSHAP Importance Scores:\n", shap_importance)
print("\nLIME Importance Scores:\n", lime_importance)

# =============================================================================
# 6️⃣ Graph Visualizations
# =============================================================================
# Graph Structure Visualization
plt.figure(figsize=(10, 6))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_size=700, node_color='skyblue', edge_color='gray')
nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): d.get('weight', 0) for u, v, d in G.edges(data=True)})
plt.title("Network Graph")
plt.show()

# Centrality Measures Visualization
plt.figure(figsize=(12, 6))
centrality_df.set_index('Node').plot(kind='bar', figsize=(12, 6))
plt.title("Centrality Measures")
plt.ylabel("Score")
plt.xlabel("Nodes")
plt.xticks(rotation=45)
plt.legend()
plt.show()

# Trust Scores Histogram
plt.figure(figsize=(8, 5))
sns.histplot(trust_df['Final Trust Score'], bins=10, kde=True, color='blue')
plt.title("Distribution of Trust Scores")
plt.xlabel("Trust Score")
plt.ylabel("Frequency")
plt.show()

# Federated Learning Loss Visualization
plt.figure(figsize=(8, 5))
sns.lineplot(x='Round', y='Client 0 Loss', data=fed_learning_df, label='Client 0 Loss', marker='o')
sns.lineplot(x='Round', y='Client 1 Loss', data=fed_learning_df, label='Client 1 Loss', marker='o')
plt.title("Federated Learning Loss per Round")
plt.xlabel("Round")
plt.ylabel("Loss")
plt.legend()
plt.show()

# SHAP & LIME Importance Bar Charts
fig, ax = plt.subplots(1, 2, figsize=(14, 6))
shap_importance.T.plot(kind='bar', ax=ax[0], title='SHAP Feature Importance', legend=False)
lime_importance.T.plot(kind='bar', ax=ax[1], title='LIME Feature Importance', legend=False)
plt.show()

import numpy as np
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import GCNConv
import shap
from lime import lime_tabular

# 1️⃣ Load and Preprocess Data
df = pd.read_csv("occupation_data_one_hot_encoded.csv")

# Extract feature vectors (age + occupation + gender)
features = df.drop(columns=['age']).values  # Use one-hot encoded features
ages = df['age'].values.reshape(-1, 1)

# Normalize age (0-1 scaling)
ages_normalized = (ages - np.min(ages)) / (np.max(ages) - np.min(ages))
feature_matrix = np.concatenate([ages_normalized, features], axis=1)

# 2️⃣ Build Social Graph
G = nx.Graph()
num_users = len(feature_matrix)
threshold = 0.5

# Add nodes
for i in range(num_users):
    G.add_node(i)

# Compute cosine similarities and add edges
for i in range(num_users):
    for j in range(i+1, num_users):
        similarity = np.dot(feature_matrix[i], feature_matrix[j]) / (
            np.linalg.norm(feature_matrix[i]) * np.linalg.norm(feature_matrix[j]))
        if similarity >= threshold:
            G.add_edge(i, j, weight=similarity)

# 3️⃣ Compute Centrality Measures
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)

# Create feature matrix with centrality measures
centrality_features = np.array([
    [degree_centrality[node],
     betweenness_centrality[node],
     closeness_centrality[node],
     eigenvector_centrality[node]]
    for node in G.nodes
])

# 4️⃣ SIR Simulation for Trust Labels
def sir_simulation(G, steps=10, infection_rate=0.1, recovery_rate=0.05):
    states = {node: 0 for node in G.nodes}  # 0=Susceptible
    initial_infected = np.random.choice(list(G.nodes))
    states[initial_infected] = 1  # 1=Infected

    for _ in range(steps):
        new_states = states.copy()
        for node in G.nodes:
            if states[node] == 1:
                # Infect neighbors
                for neighbor in G.neighbors(node):
                    if states[neighbor] == 0 and np.random.rand() < infection_rate:
                        new_states[neighbor] = 1
                # Recover
                if np.random.rand() < recovery_rate:
                    new_states[node] = 2  # 2=Recovered
        states = new_states
    return states

sir_results = sir_simulation(G)
trust_labels = torch.tensor([1 if sir_results[node] == 2 else 0 for node in G.nodes], dtype=torch.float32)

# 5️⃣ GCN Model
class TrustGCN(nn.Module):
    def __init__(self, input_dim=4, hidden_dim=8):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, 1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = self.conv2(x, edge_index)
        return x.squeeze()

# Prepare graph data
edge_index = torch.tensor(list(G.edges)).t().contiguous()
features_tensor = torch.tensor(centrality_features, dtype=torch.float32)

# 6️⃣ Train the Model
model = TrustGCN()
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for epoch in range(100):
    optimizer.zero_grad()
    preds = model(features_tensor, edge_index)
    loss = loss_fn(preds, trust_labels)
    loss.backward()
    optimizer.step()

# 7️⃣ Explainability with SHAP
def model_wrapper(x):
    return model(torch.tensor(x, dtype=torch.float32), edge_index).detach().numpy()

explainer = shap.Explainer(model_wrapper, centrality_features)
shap_values = explainer(centrality_features)
shap.summary_plot(shap_values, centrality_features, feature_names=["Degree", "Betweenness", "Closeness", "Eigenvector"])

# 8️⃣ LIME Explanations
explainer_lime = lime_tabular.LimeTabularExplainer(
    centrality_features,
    feature_names=["Degree", "Betweenness", "Closeness", "Eigenvector"],
    mode="regression"
)

# Explain first user
exp = explainer_lime.explain_instance(
    centrality_features[0],
    model_wrapper,
    num_features=4
)
exp.show_in_notebook()

# 9️⃣ Access Control
trust_scores = model(features_tensor, edge_index).detach().numpy()
threshold = 0.7

decisions = {node: "Granted" if score >= threshold else "Denied"
            for node, score in enumerate(trust_scores)}

print("\nAccess Control Decisions:")
for node, decision in decisions.items():
    print(f"User {node}: Access {decision}")

import numpy as np
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

# 1️⃣ Load and Preprocess Data
df = pd.read_csv("occupation_data_one_hot_encoded.csv")
features = df.drop(columns=['age']).values
ages = df['age'].values.reshape(-1, 1)
ages_normalized = (ages - ages.min()) / (ages.max() - ages.min())
feature_matrix = np.concatenate([ages_normalized, features], axis=1)

# 2️⃣ Build Social Graph with Valid Indices
G = nx.Graph()
num_users = len(feature_matrix)

# Add nodes with valid indices
for i in range(num_users):
    G.add_node(i)

# Add edges with proper indices
threshold = 0.5
for i in range(num_users):
    for j in range(i+1, num_users):
        similarity = np.dot(feature_matrix[i], feature_matrix[j]) / (
            np.linalg.norm(feature_matrix[i]) * np.linalg.norm(feature_matrix[j]))
        if similarity >= threshold:
            G.add_edge(i, j, weight=similarity)

# 3️⃣ Verify and Fix Edge Index
# Convert to PyTorch Geometric compatible format
edges = list(G.edges())
edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

# Verify node indices
assert edge_index.max() < num_users, "Invalid node indices in edge_index"

# 4️⃣ Rest of the Implementation
# Compute centrality measures
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)
eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)

centrality_features = np.array([
    [degree_centrality[node],
    betweenness_centrality[node],
    closeness_centrality[node],
    eigenvector_centrality[node]]
    for node in G.nodes
])

# Convert to tensors
features_tensor = torch.tensor(centrality_features, dtype=torch.float32)



class TrustGCN(nn.Module):
    def __init__(self, input_dim=4, hidden_dim=8):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, 1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = self.conv2(x, edge_index)
        return torch.sigmoid(x).squeeze()  # Sigmoid activation

# Initialize with balanced loss
pos_weight = torch.tensor([5.0])  # Adjust based on label ratio
loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

# Train with augmented labels
for epoch in range(200):  # More epochs
    optimizer.zero_grad()
    preds = model(features_tensor, edge_index)
    loss = loss_fn(preds, trust_labels)
    loss.backward()
    optimizer.step()

print("Training completed successfully!")

trust_scores = model(features_tensor, edge_index).detach().numpy()
threshold = 0.005


print("\nAccess Control Decisions with Trust Scores:")
for node, score in enumerate(trust_scores):
    decision = "Granted" if score >= threshold else "Denied"
    print(f"User {node}: Access {decision} (Score: {score:.4f})")

import shap
from lime import lime_tabular
def model_wrapper(x):
    return model(torch.tensor(x, dtype=torch.float32), edge_index).detach().numpy()

explainer = shap.Explainer(model_wrapper, centrality_features)
shap_values = explainer(centrality_features)
shap.summary_plot(shap_values, centrality_features, feature_names=["Degree", "Betweenness", "Closeness", "Eigenvector"])

# 8️⃣ LIME Explanations
explainer_lime = lime_tabular.LimeTabularExplainer(
    centrality_features,
    feature_names=["Degree", "Betweenness", "Closeness", "Eigenvector"],
    mode="regression"
)

# Explain first user
exp = explainer_lime.explain_instance(
    centrality_features[0],
    model_wrapper,
    num_features=4
)
exp.show_in_notebook()
